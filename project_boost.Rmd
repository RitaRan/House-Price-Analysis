---
title: "project.Rmd"
author: "BayeStar"
date: "4/21/2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(dplyr)
library(ggplot2)
library(caret)
library(knitr)
library(mice)
library(xgboost)
library(e1071) 
```

## Part I: Simple Model
In the first model you are allowed only limited manipulations of the original data set to predict the sales price `price`. You are allowed to take power transformations of the original variables [square roots, logs, inverses, squares, etc.] but you are NOT allowed to create interaction variables. This means that a variable may only be used once in an equation [if you use $ x^2$ donâ€™t use $x$]. Additionally, you may eliminate any data points you deem unfit. This model should have a minimum r-square of 73% (in the original units) and contain at least 6 variables but fewer than 20.   
```{r read-data}
load("ames_train.Rdata")

```


### Clean Data

* Some variables with a lot of NAs are categorical variables. After checking the data, we found the reason for having so many NAs is because the properties do not have the corresponding features. In order to keep as much information as possible, we change those NAs as new levels. For example, NAs in `Alley` are recoded as "No alley access". The variables that are dealt with this way include `Bsmt.Qual`, `Bsmt.Cond`, `BsmtFin.Type.1`, `BsmtFin.Type.2`, `Bsmt.Exposure`, `Bsmt.Full.Bath`, `Bsmt.Half.Bath`, `Fireplace.Qu`, `Garage.Type`, `Garage.Finish`, `Garage.Qual`, `Garage.Cond`, `Fence`, `Misc.Feature`, `Mas.Vnr.Type`, `Mas.Vnr.Area`, ``

* The second issue we found in the data is that new levels are identified in the test data, which causes error messages when making predictions. After using `table()` to check the new levels, we found the new levels only contain one data point, so we decided to classify the the new level to its closest class. For example, `Kitchen.Qual` contains 1 "Po"(poor) in the test data which is a new level, so we reclassify this data point as "Fa"(fair) which is its closest class. The variables that are dealt with this way include `Kitchen.Qual`, `Heating.QC`, `Electrical`, `Condition.2`, `Neighborhood`.

* The third issue we found is that `Lot.Frontage` and `Garage.Yr.Blt` are two continuous variables with a lot of NAs. We are not able to create a new level for the NAs as what we did to the categorical variables. After checking the codebook, we found `Lot.Fontage` might not be useful in predicting the price, which is also proved to be unimportant when we try to fit the boosting model, so we simply droped this variable.

* Lastly, we created a few new categorical variables for interaction.

### Eliminate Outliers

* Based on the ordered boxplots of `price` vs. `Neighborhood`(showed below), we found the two most expensive neighborhoods "NridgHt" and "NoRidge" have four properties with extremely high prices(>$580,000). Since they are far away from the main price cluster, we decided to treat them as outliers and remove them. We also check the other neighborhoods and found that the neighborhood "Gilbert" and "NAmes" also have prices extremely high. Those prices are also removed.

```{r}
# price v.s. neighborhood
ggplot(ames_train, aes(x=reorder(Neighborhood, price, FUN=median), y=price))+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  geom_boxplot()+
  xlab('')+
  ylab('Price')+
  ggtitle("Price by Neighborhood")
```

### Transform Data and Fit Model

* Transforamtion of response variable `price`:
We first fitted a simplest model called `simple_model` to identify possible transformations of response variable `price`. The diagnostic plots show below indicate a non-constant variance of the residuals, which means a transformation is needed for `price`. With the help of `boxcox()` function and the plot produced below, we found `price` needs a log transformation. 

* Model build and model selection:
We first putted in a few variables that are intuitively important as out base model and kept all the variables put in significant. The variables we chose include `TotalSq`, `Year.Built`, `Garage.Area`, `Overall.Qual`, `Kitchen.Qual`, `Garage.Cond`, etc. Based on this base model, forward selection was used to select more variables that can be included in our model and improve the prediction accuracy. In this process we try to avoid the variables might be correlated with other variables, such as TotalSq an area. With the help of the diagonostic plots, we also removed some outliers, and finally end up with a model with 20 variables.$$\\\\$$

We compared our base model and models with selected added terms from stepwise selection, then used anova to show the significance of adding those variables. All of the added variables are significant, which leads our to our final initial model. 

Based on the summary table of the selected model, all the selected continuous variables are extremely significant. These continuous variables include `area`, `Year.Built`, `Year.Remod.Add`, `Garage.Area`, `Overall.Qual`, `Lot.Area`, `BsmtFin.SF.1`, `Overall.Cond`, `Total.Bsmt.SF`, `Bsmt.Full.Bath` and `Screen.Porch`. Additionally, some categorical variables having a lot of significant levels are selected, such as `Neighborhood`, `Kitchen.Qual`, `Exter.Qual`.$$\\\\$$

Explanation of coeficients:

a. Area is the most significant continuous variable in the model with a coefficient of 1.373e-03, which indicates when the other conditions stay the same, one unit increase in area leads to exp(2.702e-04)=1.0002 times of original price. The reason for having such a small increasing ratio is that the 1.0002 times a price with a large magnitude can still lead to a big increment.  
b. Kichen.Qual if the most significant categorical variable. The base case is level "Ex". Level "Gd" has a coefficient of -5.298e-02, which means properties with a good quality kitchen have prices exp(-5.298e-02)=0.948 times of the prices of properties with an excellent quality kitchen. Level "TA" and "Fa" have lower ratios 0.9296 and 0.9218 respectively, which indicates properties with a better kitchen will have a higher price. 

The full R code and analysis is shown below.
```{r}
# clean data
clean_data = function(xdata){
xdata %>%
    mutate(# replace NAs with new levels
           Alley = as.factor(ifelse(is.na(as.character(Alley)), 
                                    "No alley access", as.character(Alley))),
           Bsmt.Qual = as.factor(ifelse(as.character(Bsmt.Qual)=="Po", 
                                           "Fa", as.character(Bsmt.Qual))),
           Bsmt.Qual = as.factor(ifelse(is.na(as.character(Bsmt.Qual)), 
                                           "No Basement", as.character(Bsmt.Qual))),
           Bsmt.Cond = as.factor(ifelse(is.na(as.character(Bsmt.Cond)), 
                                           "No Basement", as.character(Bsmt.Cond))),
           BsmtFin.Type.1 = as.factor(ifelse(is.na(as.character(BsmtFin.Type.1)), 
                                           "No Basement", as.character(BsmtFin.Type.1))),
           BsmtFin.Type.2 = as.factor(ifelse(is.na(as.character(BsmtFin.Type.2)), 
                                           "No Basement", as.character(BsmtFin.Type.2))),
           Bsmt.Exposure = as.factor(ifelse(is.na(as.character(Bsmt.Exposure))|
                                              as.character(Bsmt.Exposure) == "", 
                                           "No Basement", as.character(Bsmt.Exposure))),
           Bsmt.Unf.Rate.SF = ifelse(Total.Bsmt.SF!=0, Bsmt.Unf.SF/Total.Bsmt.SF, 0),
           Bsmt.Full.Bath = ifelse(is.na(Bsmt.Full.Bath),0,Bsmt.Full.Bath),
           Bsmt.Half.Bath = ifelse(is.na(Bsmt.Half.Bath),0,Bsmt.Half.Bath),
           Fireplace.Qu = as.factor(ifelse(is.na(as.character(Fireplace.Qu)), 
                                           "No Fireplace", as.character(Fireplace.Qu))),
           Garage.Type = as.factor(ifelse(is.na(as.character(Garage.Type)),
                                           "No Garage", as.character(Garage.Type))),
           Garage.Finish = as.factor(ifelse(is.na(as.character(Garage.Finish))|
                                              as.character(Garage.Finish) == "",
                                           "No Garage", as.character(Garage.Finish))),
           Garage.Qual = as.factor(ifelse(as.character(Garage.Qual)=="Ex", 
                                          "Gd", as.character(Garage.Qual))),
           Garage.Qual = as.factor(ifelse(is.na(as.character(Garage.Qual)),
                                           "No Garage", as.character(Garage.Qual))),
           Garage.Cond = as.factor(ifelse(as.character(Garage.Cond)=="Ex", 
                                          "Gd", as.character(Garage.Cond))),
           Garage.Cond = as.factor(ifelse(is.na(as.character(Garage.Cond))|as.character(Garage.Cond)=="Po",
                                           "No Garage", as.character(Garage.Cond))),
           # deal with new level issue in test data
           Fence = as.factor(ifelse(is.na(as.character(Fence)),
                                           "No Fence", as.character(Fence))),
           Misc.Feature = as.factor(ifelse(is.na(as.character(Misc.Feature)),
                                           "None", as.character(Misc.Feature))),
           Mas.Vnr.Type = as.factor(ifelse(as.character(Mas.Vnr.Type) == "",
                                           "None", as.character(Mas.Vnr.Type))),
           Mas.Vnr.Area = ifelse(is.na(Mas.Vnr.Area),0,Mas.Vnr.Area),
           Kitchen.Qual = as.factor(ifelse(as.character(Kitchen.Qual)=="Po", 
                                           "Fa", as.character(Kitchen.Qual))),
           Heating.QC = as.factor(ifelse(as.character(Heating.QC)=="Po", 
                                         "Fa", as.character(Heating.QC))),
           Electrical = as.factor(ifelse(as.character(Electrical) == "", 
                                         "SBrkr", as.character(Electrical))),
           Condition.2 = as.factor(ifelse(as.character(Condition.2) %in%
                                            c("Artery","RRAn","RRAe"),
                                          "Feedr", as.character(Condition.2))),
           Neighborhood = as.factor(ifelse(as.character(Neighborhood)=="Blueste",
                                          "NPkVill", as.character(Neighborhood))),
           # create new variables
           Enclosed.Porch.is = as.factor(ifelse(Enclosed.Porch==0,"N","Y")),
           Pool.Area = as.factor(ifelse(Pool.Area==0,"N", "Y")),
           Garage.Yr.Blt = ifelse(is.na(Garage.Yr.Blt), Year.Built-2, Garage.Yr.Blt)
          )%>%
    dplyr::select(-c(Lot.Frontage,Pool.QC,Pool.Area))
}

# remove outliers
ames_train = clean_data(ames_train)
ames_train = ames_train[ames_train$price<500000,-1]
remove_idx1 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("Gilbert")&ames_train$price>350000]
remove_idx2 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("NAmes")&ames_train$price>300000]
remove_idx3 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("Landmrk","GrnHill")]
ames_train = ames_train[-c(remove_idx1, remove_idx2, remove_idx3),]
```

```{r}
simple_model = lm(price ~ area, data = ames_train)
par(mfrow = c(2,2)); plot(simple_model)
```

```{r}
boxcox(simple_model)
```

```{r}
# fit base model and compare
base = lm(log(price) ~ X1st.Flr.SF + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood, data=ames_train)
summary(base)
par(mfrow = c(2,2));plot(base)

base1 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood + log(Lot.Area) , data=ames_train)

base2 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood + log(Lot.Area) + BsmtFin.SF.1, data=ames_train)

base3 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood + log(Lot.Area) + BsmtFin.SF.1 + Overall.Cond , data=ames_train)

base4 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood + log(Lot.Area) + BsmtFin.SF.1 +  Overall.Cond + Total.Bsmt.SF, data=ames_train)

base5 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood + log(Lot.Area) + BsmtFin.SF.1 +  Overall.Cond + Total.Bsmt.SF + Central.Air, data=ames_train)

model1 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood +log(Lot.Area) + BsmtFin.SF.1 + Overall.Cond + Total.Bsmt.SF + Central.Air + Bsmt.Full.Bath + Screen.Porch  + Exter.Qual + Bldg.Type + Bsmt.Qual + Garage.Cond  + Heating.QC, data=ames_train)

anova(base, base1, base2, base3, base4, base5, model1)

# final model
ames_train = ames_train[-c(167,460,183),]
model1 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood +log(Lot.Area) + BsmtFin.SF.1 + Overall.Cond + Total.Bsmt.SF + Central.Air + Bsmt.Full.Bath + Screen.Porch  + Exter.Qual + Bldg.Type + Bsmt.Qual + Garage.Cond  + Heating.QC, data=ames_train)
summary(model1)

par(mfrow = c(2,2));plot(model1)
```

### Evaluate Model on Test Data

Create predicted values for price using your model using the testing data

```{r read-test-data}
load("ames_test.Rdata")
```

```{r predict-model1, echo=FALSE}
# clean test data
ames_test = clean_data(ames_test)
# prediction
Yhat1 = predict(model1, newdata=ames_test, interval = "pred")
# change back to original units
Yhat1 = exp(Yhat1)
```

```{r}
# test criteria 
rmse = function(y, yhat){
  sqrt(mean((y-yhat)^2))
}

bias = function(yhat, y){
  mean(yhat-y)
}
maxDeviation = function(yhat, y){
  max(abs(yhat-y))
}
meanDeviation = function(yhat, y){
  mean(abs(yhat-y))
}
coverage = function(y, lwr, upr){
  mean(y>=lwr && y<=upr)
}
# evaluation 
rmse1 = rmse(Yhat1[,1], ames_test$price)
bias1 = bias(Yhat1[,1], ames_test$price)
maxDeviation1 = maxDeviation(Yhat1[,1], ames_test$price)
meanDeviation1 = meanDeviation(Yhat1[,1], ames_test$price)
coverage1 = coverage(ames_test$price, Yhat1[,2], Yhat1[,3])
```


### Model Checking
_Model Check_ - Test your prediction on the first observation in the training and test data set to make sure that the model gives a reasonable answer and include this in a supplement of your report. This should be done BY HAND using a calculator (this means use the raw data from the original dataset and manually calculate all transformations and interactions with your calculator)! Models that do not give reasonable answers will be given a minimum 2 letter grade reduction. Also be careful as you cannot use certain transformations [log or inverse x] if a variable has values of 0.

Based on our model above, we calculated prediction for the first observation in both the training and test data. For the training data, the prediction for the first observation (with PID 526354020) is 11.78. After converting it to original unit, we got 130614, which is very close to the true value 137000. For the test data, the prediction for the first observation is 12.21, which is 200787. This is also a reasonable prediction since the true value is 192100. 

## Part II: Complex Model

In this part you may go all out for constructing a best fitting model for predicting housing prices using methods that we have covered this semester.  You should feel free to to create any new variables (such as quadratic, interaction, or indicator variables, splines, etc). The variable `TotalSq = X1st.Flr.SF+X2nd.Flr.SF` was added to the dataframe (that does not include basement area, so you may improve on this. A relative grade is assigned by comparing your fit on the test set to that of your fellow students with bonus points awarded to those who substantially exceed their fellow students and point reductions occurring for models which fit exceedingly poorly.  

Update your predictions using your complex model to provide point estimates and CI.

You may iterate here as much as you like exploring different models until you are satisfied with your results.

### Data Cleaning
1. Variable Selection
We selected a subgroup of variables out of all the predictors by the following criterion.

* Some variables are highly imbalanced. For example, `Utilities` has three categories, but one of the category has 99% of the observations in it. There is small variation and little information within these variables. Thus, these variables are out of our consideration. 
* Some variables have a large proportion of missing values. For instance, `Alley` has 1395 missing values, consisting of 93% of the training data. We remove these variables since they are noninformative and may cause damage to our models.
* There is also multicolinearity in the training data. For example, `area` and `TotalSq` have a correlation around 0.99. Since To avoid multicolinearity, we will not consider `area` in our models. 

After removing imbalanced, missing, and correlated variables, we ended up with 37 predictors. We built our models based on these predictors. 

2. Data Transformation 
Based on the density plots of continuous variables, we found out that some of them are extremely skewed. We took log transformation so that they become more bell shaped. One problem of log transformation is that original zero values change to negative infinity. In order to avoid infinity values, we add 1 unit to those variables with zeros values.

### Model Fitting
We decided to use Gradient Boosting for linear regression to fit the data. We checked the data and splitted the training set into qualitative variables and quantitative variables. And we found out that among all qualitative ones, 12 have near zero variance, which indicate that the frequency of the most common value to the frequency of the second most common value is large. In this case, we deleted those variables. Among all quantitative variables, 8 have near zero variance, and thus we removed them. Since we only chose quantitaive variables which have no missing values,there is no need to do imputation. Here, we planned to use gibbs sampler and PCA to do imputation. Since imputation over qualitative variables introduces bias, we thus simply converted NAs into a new factor.
Then, we need to convert qualitative predictors to dummy variables in order to use XGBoost model.

```{r}
# reload data 
load("ames_train.Rdata")
load("ames_test.Rdata")
train_PID = ames_train$PID
test_PID = ames_test$PID
```

```{r}
clean_data = function(ames_train){
ames_train = ames_train %>% mutate(Alley = as.factor(ifelse(is.na(as.character(Alley)), 
                                   "No alley access", as.character(Alley))),
           Bsmt.Qual = as.factor(ifelse(is.na(as.character(Bsmt.Qual)), 
                                           "No Basement", as.character(Bsmt.Qual))),
           Bsmt.Cond = as.factor(ifelse(is.na(as.character(Bsmt.Cond)), 
                                           "No Basement", as.character(Bsmt.Cond))),
           BsmtFin.Type.1 = as.factor(ifelse(is.na(as.character(BsmtFin.Type.1)), 
                                           "No Basement", as.character(BsmtFin.Type.1))),
           BsmtFin.Type.2 = as.factor(ifelse(is.na(as.character(BsmtFin.Type.2)), 
                                           "No Basement", as.character(BsmtFin.Type.2))),
           Bsmt.Exposure = as.factor(ifelse(is.na(as.character(Bsmt.Exposure))|
                                              as.character(Bsmt.Exposure) == "", 
                                           "No Basement", as.character(Bsmt.Exposure))),
           Bsmt.Unf.Rate.SF = ifelse(Total.Bsmt.SF!=0, Bsmt.Unf.SF/Total.Bsmt.SF, 0),
           Bsmt.Full.Bath = ifelse(is.na(Bsmt.Full.Bath),0,Bsmt.Full.Bath),
           Bsmt.Half.Bath = ifelse(is.na(Bsmt.Half.Bath),0,Bsmt.Half.Bath),
           Fireplace.Qu = as.factor(ifelse(is.na(as.character(Fireplace.Qu)), 
                                           "No Fireplace", as.character(Fireplace.Qu))),
           Garage.Type = as.factor(ifelse(is.na(as.character(Garage.Type)),
                                           "No Garage", as.character(Garage.Type))),
           Garage.Finish = as.factor(ifelse(is.na(as.character(Garage.Finish))|
                                              as.character(Garage.Finish) == "",
                                           "No Garage", as.character(Garage.Finish))),
           Garage.Qual = as.factor(ifelse(is.na(as.character(Garage.Qual)),
                                           "No Garage", as.character(Garage.Qual))),
           Garage.Cond = as.factor(ifelse(is.na(as.character(Garage.Cond)),
                                           "No Garage", as.character(Garage.Cond))),
           Pool.Area = as.factor(ifelse(Pool.Area==0,
                                           "N", "Y")),
           Fence = as.factor(ifelse(is.na(as.character(Fence)),
                                           "No Fence", as.character(Fence))),
          Misc.Feature = as.factor(ifelse(is.na(as.character(Misc.Feature)),
                                           "None", as.character(Misc.Feature))),
           Mas.Vnr.Type = as.factor(ifelse(as.character(Mas.Vnr.Type) == "",
                                           "None", as.character(Mas.Vnr.Type))),
           Mas.Vnr.Area = ifelse(is.na(Mas.Vnr.Area),0,Mas.Vnr.Area),
           Kitchen.Qual = as.factor(ifelse(as.character(Kitchen.Qual)=="Po", 
                                           "Fa", as.character(Kitchen.Qual))),
           Heating.QC = as.factor(ifelse(as.character(Heating.QC)=="Po", 
                                         "Fa", as.character(Heating.QC))),
           Garage.Cond = as.factor(ifelse(as.character(Garage.Cond)=="Ex", 
                                          "Gd", as.character(Garage.Cond))),
           Garage.Qual = as.factor(ifelse(as.character(Garage.Qual)=="Ex", 
                                          "Gd", as.character(Garage.Qual))),
           Electrical = as.factor(ifelse(as.character(Electrical) == "", 
                                         "SBrkr", as.character(Electrical))),
           Condition.2 = as.factor(ifelse(as.character(Condition.2) == "Artery",
                                          "Feedr", as.character(Condition.2))
           ))

ames_train = ames_train %>%
  mutate(price = price,
         area = log(area),
         Lot.Area = log(Lot.Area),
         Mas.Vnr.Area = log(Mas.Vnr.Area+1),
         BsmtFin.SF.1 = log(BsmtFin.SF.1+1),
         BsmtFin.SF.2 = log(BsmtFin.SF.2+1),
         Bsmt.Unf.SF = log(Bsmt.Unf.SF+1),
         Total.Bsmt.SF = log(Total.Bsmt.SF+1),
         X1st.Flr.SF = log(X1st.Flr.SF),
         X2nd.Flr.SF = log(X2nd.Flr.SF+1),
         Low.Qual.Fin.SF = log(Low.Qual.Fin.SF+1),
         Garage.Area = log(Garage.Area+1),
         Wood.Deck.SF = log(Wood.Deck.SF+1),
         Open.Porch.SF = log(Open.Porch.SF+1),
         Enclosed.Porch = log(Enclosed.Porch+1),
         X3Ssn.Porch = log(X3Ssn.Porch+1),
        Screen.Porch = log(Screen.Porch+1),
         Misc.Val = log(Misc.Val+1),
         TotalSq = log(TotalSq),
        Year.Built= Year.Built^2) 

ames_train= ames_train[,c("MS.SubClass","MS.Zoning","Lot.Area","Neighborhood","House.Style","Overall.Qual","Overall.Cond","Exterior.1st","Year.Remod.Add","Mas.Vnr.Area","Exter.Qual","Bsmt.Qual","BsmtFin.Type.1","Heating.QC","Full.Bath","X1st.Flr.SF","Kitchen.Qual","Fireplace.Qu","Bsmt.Exposure","Garage.Type", "Wood.Deck.SF","price","Open.Porch.SF","Mas.Vnr.Type","Foundation","BsmtFin.SF.1","Bsmt.Unf.SF","Garage.Area","Garage.Finish",'Condition.1','TotalSq',"Central.Air","Kitchen.AbvGr","Bedroom.AbvGr","X2nd.Flr.SF","Lot.Shape","Year.Built","PID")]
}
```


```{r}
remove_idx1 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("Gilbert")&ames_train$price>350000]
remove_idx2 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("NAmes")&ames_train$price>300000]
remove_idx3 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("Landmrk","GrnHill")]
ames_train = ames_train[-c(remove_idx1, remove_idx2, remove_idx3),]
ames_train = ames_train[-which(ames_train$price>350000),]
ames_train = clean_data(ames_train) %>% select(-PID)
pid = ames_test$PID
ames_test = clean_data(ames_test) %>% select(-PID)
```


```{r}
missing.summary <- sapply(ames_train, function(x) sum(is.na(x))) 
indexs.missing <- sapply(ames_train, function(x) sum(is.na(x))) > 0 
num.variable.missing <- length(missing.summary[indexs.missing])
freq.table.miss <- data.frame(Variable = names(missing.summary[indexs.missing]), Number.of.Missing = as.integer(missing.summary[indexs.missing]), Porcentage.of.Missing = as.numeric(prop.table(missing.summary[indexs.missing])) )
freq.table.miss <- freq.table.miss %>% dplyr::select(Variable:Porcentage.of.Missing) %>% arrange(desc(Number.of.Missing))
indexs <- missing.summary < 200
training <- ames_train[, indexs]
# We retain SalePrice 
SalePrice <- training$price
indexs.quantitative <- sapply(training, function(x) is.numeric(x))
# We split the train data set into quantitative variables and cualitatives variables.
training.quantitative <- training[, indexs.quantitative]
training.qualitative <- training[, !indexs.quantitative]
nzv <- nzv(training.qualitative, saveMetrics = TRUE)
training.qualitative <- training.qualitative[, !nzv$nzv]
nzv2 <- nzv(training.quantitative, saveMetrics = TRUE)
training.quantitative <- training.quantitative[, !nzv2$nzv]
training.quantitative.imputed <- training.quantitative
pre.proc <- preProcess(training.quantitative.imputed, method = c("center", "scale", "pca"), thresh = 0.9)
training.quantitative.imputed.pc <- predict(pre.proc, training.quantitative.imputed)
dummies <- dummyVars(~ ., data=training.qualitative)
training.dummies <- as.data.frame(predict(dummies, training.qualitative))
training.imputed <- cbind(training.dummies, training.quantitative.imputed.pc)
training.imputed$SalePrice <- log(SalePrice)
num.variables <- dim(training.imputed)[2]
train.xgboost <- training.imputed
```

```{r}
house.xgboost <- xgboost(data = data.matrix(train.xgboost[,-num.variables]),
                         label=data.matrix(train.xgboost[,num.variables]),
                 booster = "gblinear",
                 objective = "reg:linear",
                 max.depth = 35,
                 nround = 250,
                 lambda = 0,
                 lambda_bias = 0,
                 alpha = 0, 
                 missing=NA, 
                 verbose = 0)
```


```{r}
transform_data = function(ames_test){
testing <- ames_test[, indexs]
 indexs.quantitative <- sapply(testing, function(x) is.numeric(x))
testing.quantitative <- testing[, indexs.quantitative]
testing.qualitative <- testing[, !indexs.quantitative]
testing.qualitative <- testing.qualitative[, !nzv$nzv]
testing.quantitative <- testing.quantitative[,!nzv2$nzv] 
testing.quantitative.imputed <- testing.quantitative
testing.quantitative.imputed.pc <- predict(pre.proc, testing.quantitative.imputed)
testing.dummies <- as.data.frame(predict(dummies, testing.qualitative))
testing.imputed <- cbind(testing.dummies, testing.quantitative.imputed.pc)
return (data.matrix(testing.imputed))}
```

```{r}
pred.testing.xgboost <- predict(house.xgboost, transform_data(ames_test), missing=NA)
Yhat2 = exp(pred.testing.xgboost)

# name dataframe as predictions! DO NOT CHANGE
predictions = as.data.frame(Yhat2)
predictions$PID =pid
save(predictions, file="predict.Rdata")
```

### Model Evaluation
1. Model Evaluation
For our complex model, we plotted residuals against fitted values. All the residuals clound around zero with no particular patterns, so our model does a decent job. 
```{r}
# training dataset
pred.training.xgboost <- predict(house.xgboost, transform_data(ames_train), missing=NA)
Yhat.train = exp(pred.training.xgboost)
# residual plot
residul = ames_train$price - Yhat.train
qplot(x = Yhat.train, y = residul) +
  theme_bw()+
  xlab("Fitted Values")+
  ylab("Residual")
```

We also checked the model using the following criteria. Coverage is not available for boosting. The results of both the simple model and the complex model are shown in the table below.  

```{r}
rmse2 = rmse(Yhat2, ames_test$price)
bias2 = bias(Yhat2, ames_test$price)
maxDeviation2 = maxDeviation(Yhat2, ames_test$price)
meanDeviation2 = meanDeviation(Yhat2, ames_test$price)

res = data.frame(rmse = c(rmse1, rmse2),
                 bias = c(bias1, bias2),
                 maxDeviation = c(maxDeviation1, maxDeviation2),
                 meanDeviation = c(meanDeviation1, meanDeviation2),
                 coverage = c(coverage1, NA)
                 )
rownames(res) = c("simple model", "complex model")
kable(res)
```

2. Model Checking
Since gradient boosing is hard to compute by hand, we cannot directly use calculators to obtain house price of first observation in both train and test datasets. Instead, we used R function to calculate them. For first observation in the training data, the prediction is 143400, which is a bit greater than the true value 137000. For the training data, the first prediction is 194059, close to the true value 192100. 

3. Model Results
Top 10 undervalued and overvalued houses are shown in the tables below. The most undervalued house is the one with parcel ID 528102010. We may invest in this house and sell it after the price rises. The most overvalued house is the one with parcel ID 921128020. We may sell this house now since its value may drop in the future. 
```{r}
residual = Yhat2 - ames_test$price
ntest = dim(ames_test)[1]
nleft = ntest - 10

least_over = sort(residual, partial=nleft)[nleft]
id1 = which(residual > least_over)
df1 = data.frame(
  PID = test_PID[id1],
  Predicted.Value = Yhat2[id1],
  Real.Value = ames_test$price[id1],
  Difference = residual[id1]
)
kable(df1, caption = "Undervalued Houses")
```


```{r}
residual = ames_test$price - Yhat2
ntest = dim(ames_test)[1]
nleft = ntest - 10
id2 = which(residual > least_over)
df2 = data.frame(
  PID = test_PID[id2],
  Predicted.Value = Yhat2[id2],
  Real.Value = ames_test$price[id2],
  Difference = -residual[id2]
)
kable(df2, caption = "Overvalued Houses")
```

## Part III: Write Up

Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `writeup.Rmd` by copying over salient parts of your R notebook. The written assignment consists of five parts:

1. Exploratory data analysis (20 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

2. Development and assessment of an initial model from Part I (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection.  Interpretation of coefficients desirable for full points.

* Model selection: must include a discussion

* Residual: must include a residual plot and a discussion

* RMSE: must include an RMSE and an explanation  (other criteria desirable)

* Model testing: must include an explanation

3. Development of the final model (20 points)

* Final model: must include a summary table

* Variables: must include an explanation

* Variable selection/shrinkage: must use appropriate method and include an explanation

4. Assessment of the final model (25 points)

* Residual: must include a residual plot and a discussion

* RMSE: must include an RMSE and an explanation  (other criteria desirable)

* Model evaluation: must include an evaluation discussion

* Model testing : must include a discussion

* Model result: must include a selection of the top 10 undervalued and overvalued  houses

5. Conclusion (10 points): must include a summary of results and a discussion of things learned

The whole project is aimed at predicting prices of properties in Ames area. Based on the explanations in the codebook, we cleaned the NAs by creating new levels and then fitted a linear model as our initial model with the help of forward selection. In the process of improving our model by forward selection, we found it always included some variables with problem of colinearity. Thus before we fit our complex model, we decided to first remove the problemtic variables. The problemetic variables include variables with few unique values, variables with possible colinearity, and variables with a lot of NAs. We also identified a few continuous variables needing log transformations. Using these transformed and selected variables, we decided to use booting on linear model. The result given by booting model highly improved the prediction accuracy. After satisfied with our further pruned model, we finally predicted the properties that are overvalued and undervalued.

When the number of predictors is very large, cleaning data and removing the redundant variables by reasoning is the most crucial step for making a good prediction, and it is even more important than finding a good model, because we found that forward selcetion in such circumstance is not able to deal with the problem of colinearity well, and the accuracy of different models do not have much difference. We found removing outliers is also important for making a good prediction and improving the accuracy. Booting is always a good model to try when we want to further imporve our model.

## Part IV
Create predictions for the validation data from your final model and write out to a file `prediction-validation.Rdata`
This should have the same format as the models in Part I and II. 10 points

```{r, echo=FALSE}
load("ames_validation.Rdata")
pid_validation = ames_validation$PID
ames_validation = clean_data(ames_validation)%>% select(-PID)
ames_validation = ames_validation %>% mutate(price = as.numeric(price))
Yhat<- exp(predict(house.xgboost, transform_data(ames_validation), missing=NA))
predictions = as.data.frame(Yhat)
predictions$PID = pid_validation
save(predictions, file="prediction-validation.Rdata" )
```

## Class Presentations

Each Group should prepare 5 slides in their Github repo:  (save as slides.pdf)

* Most interesting graphic  (a picture is worth a thousand words prize!)  

* Best Model (motivation, how you found it, why you think it is best)

* Best Insights into predicting Sales Price.

* 2 Best Houses to purchase  (and why)

* Best Team Name/Graphic

We will select winners based on the above criteria and overall performance.


Finally your repo should have: `writeup.Rmd`, `writeup.pdf`, `slides.Rmd` (and whatever output you use for the presentation) and `predict.Rdata` and `predict-validation.Rdata`.