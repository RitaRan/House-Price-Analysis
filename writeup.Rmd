---
title: "STA 521 Final Project Writeup"
author: "BayeStar"
date: "April 26, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(dplyr)
library(ggplot2)
library(knitr)
```

## Exploratory data analysis

```{r}
load("ames_train.Rdata")
load("ames_test.Rdata")
```

* Figure 1 shows the boxplot of prices by each neighborhood. It is apparent that prices vary a lot by different neighborhoods. Outliers are also captured and we considered deleting some of them. 
* Figure 2 is a scatterplot of prices against kitchen quality. The left panel corresponds to houses without porch, while the right with porch. We can see that the relationship differs between with and without porch, so we decided to add a dummy variable indicating the existence of porch and an interaction between the dummy and porch area.
* Figure 3 shows the prices of houses built in various years. There is a non-linear trend, so we considered adding quadratic term of year to capture the non-linearity. 
* Figure 4 depicts the relationship between prices and total square by neighborhood. (Only four of neighborhoods are shown for plot purpose.) Prices and total square are linearly related, but the linear relationship changes across neigborhood. We considered adding interaction term of total square and neighborhood to capture different slopes. 
* Figure 5 is a vilion plot showing the distribution of prices by overall quality. Prices are higher, but also vary more for higher quality.  
```{r}
# price v.s. neighborhood
ggplot(ames_train, aes(x=reorder(Neighborhood, price, FUN=median), y=price))+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  geom_boxplot()+
  xlab('')+
  ylab('Price')+
  ggtitle("Figure 1. Prices by Neighborhood")

# price v.s. Kitchen.Qual
ames_train %>%
  ggplot(aes(x=Kitchen.Qual, y=price))+
  geom_boxplot(col="dark blue", cex=1, alpha=0.5)+
  xlab("Kitchen Quality")+
  ylab("Price")+
  theme_bw()+
  ggtitle("Figure 2. Prices against Kitchen Quality")

# year
ggplot(ames_train, aes(x=Year.Built, y=price,alpha = 0.5))+
  geom_point(col="dark blue", cex=1)+
  geom_smooth(method = "lm", formula = y ~ x + I(x^2)+I(x^3), 
              col="black", se=F, size=0.7)+
  theme_bw()+
  theme(legend.position="none")+
  ylab('Price')+
  xlab('')+
  ggtitle("Figure 3. Prices of Houses Built in Years")
  
# Neighborhood and TotalSq
ames_train %>%
  filter(Neighborhood=="OldTown" |Neighborhood=="NridgHt"|
           Neighborhood=="NWAmes"|Neighborhood=="StoneBr") %>%
  ggplot(aes(x=TotalSq, y=price))+
  geom_point(col="dark blue", cex=1, alpha=0.5)+
  facet_wrap(~Neighborhood)+
  geom_smooth(method="lm", se=F, col="black", size=0.7)+
  theme_bw()+
  xlab("Total Square")+
  ylab("Price")+
  ggtitle("Figure 4. Prices by Total Square in Different Neighborhoods")

# Overall.Qual
ggplot(ames_train, aes(x=factor(Overall.Qual), y=price))+
  geom_violin(aes(color=factor(Overall.Qual), 
                  fill = factor(Overall.Qual), alpha=0.5))+
  theme_bw() +
  theme(legend.position="none")+
  xlab("Overal Quality")+
  ylab("Price")+
  ggtitle("Figure 5. Prices by Overall Quality")
```

## Simple Model 
#### Data Cleaning
```{r}
# clean data
clean_data = function(xdata){
xdata %>%
    mutate(# replace NAs with new levels
           Alley = as.factor(ifelse(is.na(as.character(Alley)), 
                                    "No alley access", as.character(Alley))),
           Bsmt.Qual = as.factor(ifelse(as.character(Bsmt.Qual)=="Po", 
                                           "Fa", as.character(Bsmt.Qual))),
           Bsmt.Qual = as.factor(ifelse(is.na(as.character(Bsmt.Qual)), 
                                           "No Basement", as.character(Bsmt.Qual))),
           Bsmt.Cond = as.factor(ifelse(is.na(as.character(Bsmt.Cond)), 
                                           "No Basement", as.character(Bsmt.Cond))),
           BsmtFin.Type.1 = as.factor(ifelse(is.na(as.character(BsmtFin.Type.1)), 
                                           "No Basement", as.character(BsmtFin.Type.1))),
           BsmtFin.Type.2 = as.factor(ifelse(is.na(as.character(BsmtFin.Type.2)), 
                                           "No Basement", as.character(BsmtFin.Type.2))),
           Bsmt.Exposure = as.factor(ifelse(is.na(as.character(Bsmt.Exposure))|
                                              as.character(Bsmt.Exposure) == "", 
                                           "No Basement", as.character(Bsmt.Exposure))),
           Bsmt.Unf.Rate.SF = ifelse(Total.Bsmt.SF!=0, Bsmt.Unf.SF/Total.Bsmt.SF, 0),
           Bsmt.Full.Bath = ifelse(is.na(Bsmt.Full.Bath),0,Bsmt.Full.Bath),
           Bsmt.Half.Bath = ifelse(is.na(Bsmt.Half.Bath),0,Bsmt.Half.Bath),
           Fireplace.Qu = as.factor(ifelse(is.na(as.character(Fireplace.Qu)), 
                                           "No Fireplace", as.character(Fireplace.Qu))),
           Garage.Type = as.factor(ifelse(is.na(as.character(Garage.Type)),
                                           "No Garage", as.character(Garage.Type))),
           Garage.Finish = as.factor(ifelse(is.na(as.character(Garage.Finish))|
                                              as.character(Garage.Finish) == "",
                                           "No Garage", as.character(Garage.Finish))),
           Garage.Qual = as.factor(ifelse(as.character(Garage.Qual)=="Ex", 
                                          "Gd", as.character(Garage.Qual))),
           Garage.Qual = as.factor(ifelse(is.na(as.character(Garage.Qual)),
                                           "No Garage", as.character(Garage.Qual))),
           Garage.Cond = as.factor(ifelse(as.character(Garage.Cond)=="Ex", 
                                          "Gd", as.character(Garage.Cond))),
           Garage.Cond = as.factor(ifelse(is.na(as.character(Garage.Cond))|
                                            as.character(Garage.Cond)=="Po",
                                           "No Garage", as.character(Garage.Cond))),
           # deal with new level issue in test data
           Fence = as.factor(ifelse(is.na(as.character(Fence)),
                                           "No Fence", as.character(Fence))),
           Misc.Feature = as.factor(ifelse(is.na(as.character(Misc.Feature)),
                                           "None", as.character(Misc.Feature))),
           Mas.Vnr.Type = as.factor(ifelse(as.character(Mas.Vnr.Type) == "",
                                           "None", as.character(Mas.Vnr.Type))),
           Mas.Vnr.Area = ifelse(is.na(Mas.Vnr.Area),0,Mas.Vnr.Area),
           Kitchen.Qual = as.factor(ifelse(as.character(Kitchen.Qual)=="Po", 
                                           "Fa", as.character(Kitchen.Qual))),
           Heating.QC = as.factor(ifelse(as.character(Heating.QC)=="Po", 
                                         "Fa", as.character(Heating.QC))),
           Electrical = as.factor(ifelse(as.character(Electrical) == "", 
                                         "SBrkr", as.character(Electrical))),
           Condition.2 = as.factor(ifelse(as.character(Condition.2) %in%
                                            c("Artery","RRAn","RRAe"),
                                          "Feedr", as.character(Condition.2))),
           Neighborhood = as.factor(ifelse(as.character(Neighborhood)=="Blueste",
                                          "NPkVill", as.character(Neighborhood))),
           # create new variables
           Enclosed.Porch.is = as.factor(ifelse(Enclosed.Porch==0,"N","Y")),
           Pool.Area = as.factor(ifelse(Pool.Area==0,"N", "Y")),
           Garage.Yr.Blt = ifelse(is.na(Garage.Yr.Blt), Year.Built-2, Garage.Yr.Blt)
          )%>%
    dplyr::select(-c(Lot.Frontage,Pool.QC,Pool.Area))
}

# remove outliers
ames_train = clean_data(ames_train)
ames_train = ames_train[-c(462,168,183),]
ames_train = ames_train[ames_train$price<500000,]
ames_train = ames_train[ames_train$price>50000,]
ames_train = ames_train[ames_train$X1st.Flr.SF<3500,]
ames_train = ames_train[ames_train$Kitchen.AbvGr%in%c(1,2),]
remove_idx1 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("Gilbert")&ames_train$price>350000]
remove_idx2 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("NAmes")&ames_train$price>300000]
remove_idx3 = c(1:nrow(ames_train))[ames_train$Neighborhood %in%c("Landmrk","GrnHill")]
ames_train = ames_train[-c(remove_idx1, remove_idx2, remove_idx3),]
```

#### Initial Model

* Transforamtion of the response variable `price`:
We first fitted a simplest model called `simple_model` to identify possible transformations of response variable `price`. The diagnostic plots showed below indicate a non-constant variance of the residuals, which means a transformation is needed for `price`. With the help of `boxcox()` function and the plot produced below, we found that `price` needs a log transformation. 

* Model fitting and model selection:
We first putted in a few variables that are intuitively important as out base model and kept all the variables put in significant. The variables we chose include `TotalSq`, `Year.Built`, `Garage.Area`, `Overall.Qual`, `Kitchen.Qual`, `Garage.Cond`, etc. Based on this base model, forward selection was used to select more variables that can be included in our model and improve the prediction accuracy. In this process we try to avoid the variables might be correlated with other variables, such as `TotalSq` an `area`. With the help of the diagonostic plots, we also removed some outliers, and finally end up with a model with 20 variables.$$\\\\$$ 

* Results and explanation of coeficients:
Based on the summary table of the selected model, all the selected continuous variables are extremely significant. These continuous variables include `area`, `Year.Built`, `Year.Remod.Add`, `Garage.Area`, `Overall.Qual`, `Lot.Area`, `BsmtFin.SF.1`, `Overall.Cond`, `Total.Bsmt.SF`, `Bsmt.Full.Bath` and `Screen.Porch`. Additionally, some categorical variables having a lot of significant levels are selected, such as `Neighborhood`, `Kitchen.Qual`, `Exter.Qual`.$$\\\\$$

a. Area is the most significant continuous variable in the model with a coefficient of 1.373e-03, which indicates when the other conditions stay the same, one unit increase in area leads to exp(2.702e-04)=1.0002 times of original price. The reason for having such a small increasing ratio is that the 1.0002 times a price with a large magnitude can still lead to a big increment.  
b. Kichen.Qual if the most significant categorical variable. The base case is level "Ex". Level "Gd" has a coefficient of -5.298e-02, which means properties with a good quality kitchen have prices exp(-5.298e-02)=0.948 times of the prices of properties with an excellent quality kitchen. Level "TA" and "Fa" have lower ratios 0.9296 and 0.9218 respectively, which indicates properties with a better kitchen will have a higher price. 

```{r}
model1 = lm(log(price) ~ area + Year.Built + Year.Remod.Add +
              Garage.Area + Overall.Qual +  log(Lot.Area) +
              BsmtFin.SF.1 + Overall.Cond + Total.Bsmt.SF +
              Central.Air + Bsmt.Full.Bath + Screen.Porch +
              Kitchen.Qual + Exter.Qual + Bldg.Type + Bsmt.Qual +
              Garage.Cond + Neighborhood + Heating.QC, data=ames_train)
summary(model1)
```

#### Model Selection

We compared our base model and models with selected added terms from stepwise selection, then used anova to show the significance of adding those variables. All of the added variables are significant, which leads us to our final initial model. 

```{r}
base = lm(log(price) ~ X1st.Flr.SF + Year.Built + Year.Remod.Add + 
            Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood, 
          data=ames_train)

base1 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + 
             Garage.Area + Overall.Qual + Kitchen.Qual + 
             Neighborhood + log(Lot.Area) , data=ames_train)

base2 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + 
             Garage.Area + Overall.Qual + Kitchen.Qual + 
             Neighborhood + log(Lot.Area) + BsmtFin.SF.1, 
           data=ames_train)

base3 = lm(log(price) ~ area + Year.Built + Year.Remod.Add +
             Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood + 
             log(Lot.Area) + BsmtFin.SF.1 + Overall.Cond , data=ames_train)

base4 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + 
             Garage.Area + Overall.Qual + Kitchen.Qual + Neighborhood +
             log(Lot.Area) + BsmtFin.SF.1 +  Overall.Cond + Total.Bsmt.SF,
           data=ames_train)

base5 = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area
           + Overall.Qual + Kitchen.Qual + Neighborhood + log(Lot.Area) + 
             BsmtFin.SF.1 +  Overall.Cond + Total.Bsmt.SF + Central.Air, data=ames_train)

model = lm(log(price) ~ area + Year.Built + Year.Remod.Add + Garage.Area + 
              Overall.Qual + Kitchen.Qual + Neighborhood +log(Lot.Area) + 
              BsmtFin.SF.1 + Overall.Cond + Total.Bsmt.SF + Central.Air + 
              Bsmt.Full.Bath + Screen.Porch  + Exter.Qual + Bldg.Type + 
              Bsmt.Qual + Garage.Cond  + Heating.QC, data=ames_train)

kable(anova(base,base1,base2, base3, base4, base5, model), caption = "ANOVA")
```

#### Residual Diagnostics

The residual plots shows no non-constant variance, and the qq-plot shows a good normality of the redisuals except for a few potential outliers.

```{r}
par(mfrow = c(2,2)); plot(model1)
```

#### Model Evaluation

Since log transformation was used on `price`, it needs to be transformed back to the original scale. 

```{r}
ames_test = clean_data(ames_test)
Yhat1 = predict(model, newdata=ames_test, interval = "pred")
Yhat1 = exp(Yhat1)
# test criteria 
rmse = function(y, yhat){
  sqrt(mean((y-yhat)^2))
}

bias = function(yhat, y){
  mean(yhat-y)
}
maxDeviation = function(yhat, y){
  max(abs(yhat-y))
}
meanDeviation = function(yhat, y){
  mean(abs(yhat-y))
}
coverage = function(y, lwr, upr){
  mean(y>=lwr && y<=upr)
}
# evaluation 
rmse1 = rmse(Yhat1[,1], ames_test$price)
bias1 = bias(Yhat1[,1], ames_test$price)
maxDeviation1 = maxDeviation(Yhat1[,1], ames_test$price)
meanDeviation1 = meanDeviation(Yhat1[,1], ames_test$price)
coverage1 = coverage(ames_test$price, Yhat1[,2], Yhat1[,3])
```

#### Model Checking
Based on our model above, we calculated prediction for the first observation in both the training and test data. For the training data, the prediction for the first observation (with PID 526354020) is 11.78. After converting it to original unit, we got 130614, which is very close to the true value 137000. For the test data, the prediction for the first observation is 12.21, which is 200787. This is also a reasonable prediction since the true value is 192100. 

## Complex Model

#### Model Fitting

We decided to keep using linear regression to fit the data. Variable Selection, data transformation, and interaction between variables are three steps we considered. 

1. Variable Selection

We selected a subgroup of variables out of all the predictors by the following criterion.
* Some variables are highly imbalanced. For example, `Utilities` has three categories, but one of the category has 99% of the observations in it. There is small variation and little information within these variables. Thus, these variables are out of our consideration. The fucntion `nearZeroVar()` in package `caret` was used to identify these variables. Both continuous variables and categorical variables can be identified.
* Some variables have a large proportion of missing values. For instance, `Alley` has 1395 missing values, consisting of 93% of the training data. We remove these variables since they are noninformative and may cause damage to our models.
* There is also multicolinearity in the training data. For example, `area` and `TotalSq` have a correlation around 0.99. Since To avoid multicolinearity, we will not consider `area` in our models. 

After removing imbalanced, missing, and correlated variables, we ended up with 41 predictors. We built our models based on these predictors. 

2. Data Transformation 

a. log transformation: based on the histogram plots of continuous variables, we found out that some of them are extremely skewed. We took log transformation so that they become more bell shaped. One problem of log transformation is that original zero values change to negative infinity. In order to avoid infinity values, we add 1 unit to those variables with zeros values.
b. polynomial relationships: base on the plots of price vs. some of the features, we found `Year.Built` and `Year.Remod.Add` have non-linear relationships with price. We add polynomial terms on these two variables.
c. factorize continuous variables: some continuous variables are discrete with less than 20 levels and their relationships with price are not directly linear, such as `MS.SubClass` `Overall.Cond`, we decided to factorize those variables instead of keeping them as numeric values.

3. Interactions

Interactions between variables are also identified by plotting. One significant interaction is `TotalSq` between `House.Style`, which means that for different house style, the slope of the their linear relationship with price is different. We found that the interaction variable added to the `TotalSq` is a variable that can indicate the overall feature of a property. We tried to find some other variables that can also show the overall feature of a property and then added them as interaction terms. Such variables include `Heating.QC`,`Kitchen.Qual`, etc. We also identified some other combination of variables that may have interactions. Such combinations include `Lot.Area` vs. `Lot.shape` and `Garage.Type` vs. `Garage.Area`.

```{r}
model1 = lm(log(price) ~  log(TotalSq)*(Neighborhood + House.Style +
                                          Exterior.2nd + Heating.QC + Kitchen.Qual) + 
              Condition.1 + log(Lot.Area)*Lot.Shape + 
              factor(Overall.Qual) + factor(Overall.Cond) + factor(MS.SubClass)+
              poly(Year.Built,3) + poly(Year.Remod.Add,2) + 
              Bsmt.Exposure + Bsmt.Qual + Bsmt.Full.Bath + BsmtFin.Type.1 + 
              BsmtFin.SF.1 +Bsmt.Unf.SF  +  
              Garage.Type*(Garage.Area) + Garage.Finish + 
              Exterior.1st + Foundation + Functional + Street + 
              Bedroom.AbvGr + Full.Bath + Half.Bath + Kitchen.AbvGr +
              Fireplace.Qu + Fireplaces + Lot.Config + Fence + 
              Wood.Deck.SF + log(Open.Porch.SF+1) + log(Screen.Porch+1) +
              Roof.Style + Mas.Vnr.Type + Mas.Vnr.Area, data=ames_train)
```


```{r, warning=F}
Yhat.test = predict(model1, newdata=ames_test, interval = "pred")
Yhat.test = exp(Yhat.test)
Yhat.train = predict(model1, newdata=ames_train, interval = "pred")
Yhat.train = exp(Yhat.train)
```

#### Model Summary

Summary table of the complex model:
```{r}
summaries = summary(model1)
df = summaries$coefficients[c("(Intercept)","log(Lot.Area)",
                              "poly(Year.Built, 3)1","poly(Year.Built, 3)2",
                              "poly(Year.Built, 3)3","poly(Year.Remod.Add, 2)1",
                              "poly(Year.Remod.Add, 2)2",
                          "Bsmt.ExposureGd","Bsmt.ExposureMn",
                          "Bsmt.Full.Bath","Bsmt.Unf.SF",
                          "StreetPave","Full.Bath","Half.Bath",
                          "Wood.Deck.SF","log(Open.Porch.SF + 1)",
                          "log(Screen.Porch + 1)"),]
kable(df, caption = "Model Summary")
```

We picked 41 features and fit a simple linear regression model. Among them we picked some whose p-value are most significant as the table shows above.  We can conclude that area of the lot, the year to build, the year to renew, basement and bathroom conditions and etc. all play key roles in determining house prices. 

Furthermore, we investigated that the lot area and house prices are postively correlated: 10 percent increase in lot area will lead to $1.1^{6.975125} -1$ (around 90 percent) increase in price. Moreover, we found out that bathroom conditions and area of wood deck, open porch and screen porch all have positive correlation with price. a 1-unit increase in the area of wood deck leads to $e^{0.1262579}$ (around 1.134575) increase in price. Last but not least, 10 percent increase in areas of open porch and screen porch will lead to $1.1^{0.0038} -1$ (around 0.03 percent) and $1.1^{0.0062} -1$ (around 0.05 percent) increase in house prices respectively.

#### Model Evaluation
1. Residual Diagnostics

For our complex model, residuals cloud around zero with no particular patterns. There are also no points with high leverage or large Cook's distance. The only issue is that we may still have heavy tails, indicating potential outliers.  
```{r, warning=F}
par(mfrow = c(2,2)); plot(model1)
```

We also checked the model using the following criteria. The results of both the simple model and the complex model are shown in the table below. The performace of complex model greatly improves model accuracy.  

```{r}
rmse2 = rmse(Yhat.test[,'fit'], ames_test$price)
bias2 = bias(Yhat.test[,'fit'], ames_test$price)
maxDeviation2 = maxDeviation(Yhat.test[, 'fit'], ames_test$price)
meanDeviation2 = meanDeviation(Yhat.test[,'fit'], ames_test$price)
coverage2 = coverage(ames_test$price, Yhat.test[,2], Yhat.test[,3])

res = data.frame(rmse = c(rmse1, rmse2),
                 bias = c(bias1, bias2),
                 maxDeviation = c(maxDeviation1, maxDeviation2),
                 meanDeviation = c(meanDeviation1, meanDeviation2),
                 coverage = c(coverage1, coverage2)
                 )
rownames(res) = c("initial model", "complex model")
kable(res, caption = "Model Accuracy")
```

2. Model Checking

We can do model checking by using the selected features and interactions, model coefficients and intercept. The first observation in the training data, the prediction is 176547.7, which is higher than the true value 137000. For the testing data, the prediction is 190388.6, which is also close to the true value 192100.

3. Model Results

Top 10 undervalued and overvalued houses are shown in the tables below. The most undervalued house is the one with parcel ID 528102010. We may invest in this house and sell it after the price rises. The most overvalued house is the one with parcel ID 905376090. We may sell this house now since its value may drop in the future. 
```{r}
residual = Yhat.test[,1] - ames_test$price
ntest = dim(ames_test)[1]
nleft = ntest - 10
least_over = sort(residual, partial=nleft)[nleft]
id1 = which(residual > least_over)
df1 = data.frame(
  PID = ames_test$PID[id1],
  Predicted.Value = Yhat.test[id1],
  Real.Value = ames_test$price[id1],
  Difference = residual[id1]
)
kable(df1, caption = "Undervalued Houses")
```


```{r}
residual = ames_test$price - Yhat.test[,1]
ntest = dim(ames_test)[1]
nleft = ntest - 10
least_over = sort(residual, partial=nleft)[nleft]
id2 = which(residual > least_over)
df2 = data.frame(
  PID = ames_test$PID[id2],
  Predicted.Value = Yhat.test[id2],
  Real.Value = ames_test$price[id2],
  Difference = -residual[id2]
)
kable(df2, caption = "Overvalued Houses")
```

## Conclusion

Our project aims to predict houses prices using the Ames data. Based on the explanations in the codebook, we imputed some variables with missing data by giving missing values reasonable meaning and eliminated outliers. After data cleaning, we first built a linear model with variables selected from forward selection. However, after further inverstigation into these variables, we found that some of them can be excluded in the first place. Highly correlated, serveraly imbalanced, and largely missing variables not only provide little information, but also cause damage to our model. Thus, after summarising and plotting variables, we handpicked 41 of them, excluding those problematic ones. 

With these selected variables and their interactions, we built our complex model. We found that house prices are closely related to lot area, the year when the house was built and remodeled, the number of bathrooms, porch area, etc. For example, the larger the lot area, the higher the price. The more the full and half bathrooms, the higher the price. These significant variables help us to make better predictions of house prices. Our complex model does improve prediction accurarcy according to all the five evaluation criteria. With these predictions, we finally gave our suggestions on overvalued and undervalued hourses. 

We actually learnt more than we expected when working on this project. We found that variable selection is extremely important when given a number of related and messy variables. Dealing with missing values and removing outliers are also crucial for a better prediction. We were also amazed at the power of ordinary linear models. In fact, in addition to ordinary linear models, we tried Bayesian linear models, panelized linear models, tree ensembles and GAM. The ordinary linear model outperform all these models because it is computationally cheap and also able to capture non-linearity and interactions.    
